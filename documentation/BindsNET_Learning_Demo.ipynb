{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BindsNET Learning Techniques\n",
    "\n",
    "## Overview\n",
    "\n",
    "Detail documentation of usage of learning rules has been specified [here](https://bindsnet-docs.readthedocs.io/guide/guide_part_ii.html). This document will go into more specific examples of configuring a spiking neural network in BindsNET.\n",
    "\n",
    "The specified learning rule is passed into a `Connection` object via the `update_rule` argument. The connection encapsulates the learning rule object. Parameter updates are averaged across the batch dimension by default, so there is no weight decay."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre\n",
    "\n",
    "# Create two populations of neurons, one to act as the \"source\"\n",
    "# population, and the other, the \"target population\".\n",
    "# Neurons involved in certain learning rules must record synaptic\n",
    "# traces, a vector of short-term memories of the last emitted spikes.\n",
    "source_layer = Input(n=100, traces=True)\n",
    "target_layer = LIFNodes(n=1000, traces=True)\n",
    "\n",
    "# Connect the two layers.\n",
    "connection = Connection(\n",
    "    source=source_layer, target=target_layer, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## Import Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from bindsnet.encoding import *\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.monitors import Monitor\n",
    "from bindsnet.network.monitors import NetworkMonitor\n",
    "\n",
    "from bindsnet.analysis.plotting import plot_spikes, plot_voltages, plot_input, plot_weights\n",
    "\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.learning import PostPre, Hebbian, WeightDependentPostPre, MSTDP, MSTDPET\n",
    "\n",
    "from bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\n",
    "from bindsnet.utils import get_square_weights, get_square_assignments"
   ]
  },
  {
   "source": [
    "## Learning Flow\n",
    "\n",
    "1. Define Simulation Parameters\n",
    "2. Create Input Data\n",
    "3. Configure Network Architecture\n",
    "4. Define Simulation Variables\n",
    "5. Perform Learning Iterations\n",
    "6. Evaluate Classification Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simulation Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data Parameters ###\n",
    "\n",
    "# number of training samples\n",
    "training_samples = 1\n",
    "testing_samples = 10\n",
    "\n",
    "# set number of classes\n",
    "n_classes = 2\n",
    "\n",
    "### Network Configuration Parameters ###\n",
    "\n",
    "# configure number of input neurons\n",
    "input_layer_name = \"Input Layer\"\n",
    "input_neurons = 9\n",
    "\n",
    "# configure the number of output lif neurons\n",
    "lif_layer_name = \"LIF Layer\"\n",
    "lif_neurons = 2\n",
    "\n",
    "### Simulation Parameters ###\n",
    "\n",
    "# simulation time\n",
    "time = 10\n",
    "dt = 1\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 1\n",
    "\n",
    "# ratio of neurons to classes\n",
    "per_class = int(lif_neurons / n_classes)"
   ]
  },
  {
   "source": [
    "### Input Configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store unique images in a list\n",
    "imgs = []\n",
    "\n",
    "# Class 0 Image\n",
    "img0 = {\"Label\" : 0, \"Image\" : torch.FloatTensor([[1,1,1],[1,0,1],[1,1,1]])}\n",
    "imgs.append(img0)\n",
    "\n",
    "# Class 1 Image\n",
    "img1 = {\"Label\" : 1, \"Image\" : torch.FloatTensor([[0,1,0],[0,1,0],[0,1,0]])}\n",
    "imgs.append(img1)\n",
    "\n",
    "# initialize list of inputs for training\n",
    "training_dataset = []\n",
    "\n",
    "# for the number of specified training samples\n",
    "for i in range(training_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    # rand_sample = random.randint(0,n_classes-1)\n",
    "    \n",
    "    # provide an even number of training samples\n",
    "    rand_sample = i % n_classes\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    training_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# initialize the encoder\n",
    "encoder = BernoulliEncoder(time=time, dt=dt)\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_train_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in training_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_train_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})\n",
    "\n",
    "# initialize list of inputs for testing\n",
    "testing_dataset = []\n",
    "\n",
    "# for the number of specified testing samples\n",
    "for i in range(testing_samples):\n",
    "\n",
    "    # randomly select a training sample\n",
    "    rand_sample = random.randint(0,n_classes-1)\n",
    "\n",
    "    # add the sample to the list of training samples\n",
    "    testing_dataset.append(imgs[rand_sample])\n",
    "\n",
    "# list of encoded images for random selection during training\n",
    "encoded_test_inputs = []\n",
    "\n",
    "# loop through encode each image type and store into a list of encoded images\n",
    "for sample in testing_dataset:\n",
    "\n",
    "    # encode the image \n",
    "    encoded_img = encoder(torch.flatten(sample[\"Image\"]))\n",
    "\n",
    "    # encoded image input for the network\n",
    "    encoded_img_input = {input_layer_name: encoded_img}\n",
    "\n",
    "    # encoded image label\n",
    "    encoded_img_label = sample[\"Label\"]\n",
    "\n",
    "    # add to the encoded input list along with the input layer name\n",
    "    encoded_test_inputs.append({\"Label\" : encoded_img_label, \"Inputs\" : encoded_img_input})"
   ]
  },
  {
   "source": [
    "### Network Configuration\n",
    "\n",
    "When creating a connection between two layers, the learning (update) rule should be specified as well as the learning rate (nu) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "network = Network()\n",
    "\n",
    "# configure weights for the synapses between the input layer and LIF layer\n",
    "#w = torch.round(torch.abs(2 * torch.randn(input_neurons, lif_neurons)))\n",
    "w = torch.zeros(input_neurons,lif_neurons)\n",
    "\n",
    "# initialize input and LIF layers\n",
    "# spike traces must be recorded (why?)\n",
    "\n",
    "# initialize input layer\n",
    "input_layer = Input(n=input_neurons,traces=True)\n",
    "\n",
    "# initialize input layer\n",
    "lif_layer = LIFNodes(n=lif_neurons,traces=True)\n",
    "\n",
    "# initialize connection between the input layer and the LIF layer\n",
    "# specify the learning (update) rule and learning rate (nu)\n",
    "connection = Connection(\n",
    "    #source=input_layer, target=lif_layer, w=w, update_rule=PostPre, nu=(1e-4, 1e-2)\n",
    "    source=input_layer, target=lif_layer, w=w, update_rule=PostPre, nu=(1, 1)\n",
    ")\n",
    "\n",
    "# add input layer to the network\n",
    "network.add_layer(\n",
    "    layer=input_layer, name=input_layer_name\n",
    ")\n",
    "\n",
    "# add lif neuron layer to the network\n",
    "network.add_layer(\n",
    "    layer=lif_layer, name=lif_layer_name\n",
    ")\n",
    "\n",
    "# add connection to network\n",
    "network.add_connection(\n",
    "    connection=connection, source=input_layer_name, target=lif_layer_name\n",
    ")"
   ]
  },
  {
   "source": [
    "### Simulation Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the spike times of each neuron during the simulation.\n",
    "spike_record = torch.zeros(1, int(time / dt), lif_neurons)\n",
    "\n",
    "# record the mapping of each neuron to its corresponding label\n",
    "assignments = -torch.ones_like(torch.Tensor(lif_neurons))\n",
    "\n",
    "# how frequently each neuron fires for each input class\n",
    "rates = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "# the likelihood of each neuron firing for each input class\n",
    "proportions = torch.zeros_like(torch.Tensor(lif_neurons, n_classes))\n",
    "\n",
    "\n",
    "# label(s) of the input(s) being processed\n",
    "labels = torch.empty(1,dtype=torch.int)\n",
    "\n",
    "# create a spike monitor for each layer in the network\n",
    "# this allows us to read the spikes in order to assign labels to neurons and determine the predicted class \n",
    "layer_monitors = {}\n",
    "for layer in set(network.layers):\n",
    "\n",
    "    # initialize spike monitor at the layer\n",
    "    # do not record the voltage if at the input layer\n",
    "    state_vars = [\"s\",\"v\"] if (layer != input_layer_name) else [\"s\"]\n",
    "    layer_monitors[layer] = Monitor(network.layers[layer], state_vars=state_vars, time=int(time/dt))\n",
    "\n",
    "    # connect the monitor to the network\n",
    "    network.add_monitor(layer_monitors[layer], name=\"%s_spikes\" % layer)"
   ]
  },
  {
   "source": [
    "### Training\n",
    "\n",
    "Below are descriptions of the functions for evaluating the behavior of an SNN in BindsNET\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`all_activity()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"all activity\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "`proportion_weighting()`\n",
    "\n",
    "Classify data with the label with highest average spiking activity over all neurons, weighted by class-wise proportion.\n",
    "\n",
    "Returns a predictions tensor of shape `(n_samples,)` resulting from the \"proportion weighting\" classification scheme (`torch.Tensor`)\n",
    "\n",
    "----\n",
    "\n",
    "`assign_labels()`\n",
    "\n",
    "Assign labels to the neurons based on highest average spiking activity.\n",
    "\n",
    "Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates (`Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_history = None\n",
    "num_correct = 0.0\n",
    "\n",
    "### DEBUG ###\n",
    "### can be used to force the network to learn the inputs in a specific way\n",
    "supervised = True\n",
    "### used to determine if status messages are printed out at each sample\n",
    "log_messages = True\n",
    "### used to show weight changes\n",
    "plot_weights = True\n",
    "###############\n",
    "\n",
    "# iterate for epochs\n",
    "for step in range(epochs):\n",
    "    for sample in encoded_train_inputs:\n",
    "        \n",
    "        # get the label for the current image\n",
    "        labels[0] = sample[\"Label\"]\n",
    "\n",
    "        # randomly decide which output neuron should spike if more than one neuron corresponds to the class\n",
    "        # choice will always be 0 if there is one neuron per output class\n",
    "        choice = np.random.choice(per_class, size=1, replace=False)\n",
    "\n",
    "        # clamp on the output layer forces the node corresponding to the label's class to spike\n",
    "        # this is necessary in order for the network to learn which neurons correspond to which classes\n",
    "        # clamp: Mapping of layer names to boolean masks if neurons should be clamped to spiking. \n",
    "        # The ``Tensor``s have shape ``[n_neurons]`` or ``[time, n_neurons]``.\n",
    "        clamp = {lif_layer_name: per_class * labels[0] + torch.Tensor(choice).long()} if supervised else {}\n",
    "\n",
    "        print(sample[\"Inputs\"])\n",
    "\n",
    "        ### Step 1: Run the network with the provided inputs ###\n",
    "        network.run(inputs=sample[\"Inputs\"], time=time, clamp=clamp)\n",
    "\n",
    "        ### Step 2: Get the spikes produced at the output layer ###\n",
    "        spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "        \n",
    "        ### Step 3: ###\n",
    "\n",
    "        # Assign labels to the neurons based on highest average spiking activity.\n",
    "        # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "        # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "        ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "        # Classify data with the label with highest average spiking activity over all neurons.\n",
    "        all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "        ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "        ###         weighted by class-wise proportion ###\n",
    "        proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "        ### Update Accuracy\n",
    "        num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "        ######## Display Information ########\n",
    "        if log_messages:\n",
    "            print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "            \n",
    "            print(\"Neuron Label Assignments:\")\n",
    "            for idx in range(assignments.numel()):\n",
    "                print(\n",
    "                    \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                    \"Proportions:\",proportions[idx],\n",
    "                    \"Rates:\",rates[idx]\n",
    "                    )\n",
    "            print(\"\\n\")\n",
    "        #####################################\n",
    "\n",
    "\n",
    "    ### For Weight Plotting ###\n",
    "    if plot_weights:\n",
    "        weights = network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0].numpy().reshape((1,input_neurons))\n",
    "        weight_history = weights.copy() if step == 0 else np.concatenate((weight_history,weights),axis=0)\n",
    "        print(\"Neuron 0 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,0])\n",
    "        print(\"Neuron 1 Weights:\\n\",network.connections[(\"Input Layer\", \"LIF Layer\")].w[:,1])\n",
    "        print(\"====================\")\n",
    "    #############################\n",
    "\n",
    "    if log_messages:\n",
    "        print(\"Epoch #\",step,\"\\tAccuracy:\", num_correct / ((step + 1) * len(encoded_train_inputs)) )\n",
    "        print(\"===========================\\n\\n\")\n",
    "\n",
    "### For Weight Plotting ###\n",
    "# Plot Weight Changes\n",
    "if plot_weights:\n",
    "    [plt.plot(weight_history[:,idx]) for idx in range(weight_history.shape[1])]\n",
    "    plt.show()\n",
    "    \n",
    "#############################\n",
    "\n",
    "### Print Final Class Assignments and Proportions ###\n",
    "print(\"Neuron Label Assignments:\")\n",
    "for idx in range(assignments.numel()):\n",
    "    print(\n",
    "        \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "        \"Proportions:\",proportions[idx],\n",
    "        \"Rates:\",rates[idx]\n",
    "        )"
   ]
  },
  {
   "source": [
    "### Evaluate Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "\n",
    "log_messages = True\n",
    "\n",
    "# disable training mode\n",
    "network.train(False)\n",
    "\n",
    "# loop through each test example and record performance\n",
    "for sample in encoded_test_inputs:\n",
    "\n",
    "    # get the label for the current image\n",
    "    labels[0] = sample[\"Label\"]\n",
    "\n",
    "    ### Step 1: Run the network with the provided inputs ###\n",
    "    network.run(inputs=sample[\"Inputs\"], time=time)\n",
    "\n",
    "    ### Step 2: Get the spikes produced at the output layer ###\n",
    "    spike_record[0] = layer_monitors[lif_layer_name].get(\"s\").view(time, lif_neurons)\n",
    "\n",
    "    ### Step 3: ###\n",
    "\n",
    "    # Assign labels to the neurons based on highest average spiking activity.\n",
    "    # Returns a Tuple of class assignments, per-class spike proportions, and per-class firing rates \n",
    "    # Return Type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "    assignments, proportions, rates = assign_labels( spike_record, labels, n_classes, rates )\n",
    "\n",
    "    ### Step 4: Classify data based on the neuron (label) with the highest average spiking activity ###\n",
    "\n",
    "    # Classify data with the label with highest average spiking activity over all neurons.\n",
    "    all_activity_pred = all_activity(spike_record, assignments, n_classes)\n",
    "\n",
    "    ### Step 5: Classify data based on the neuron (label) with the highest average spiking activity\n",
    "    ###         weighted by class-wise proportion ###\n",
    "    proportion_pred = proportion_weighting(spike_record, assignments, proportions, n_classes)\n",
    "\n",
    "    ### Update Accuracy\n",
    "    num_correct += 1 if (labels.numpy()[0] == all_activity_pred.numpy()[0]) else 0\n",
    "\n",
    "    ######## Display Information ########\n",
    "    if log_messages:\n",
    "        print(\"Actual Label:\",labels.numpy(),\"|\",\"Predicted Label:\",all_activity_pred.numpy(),\"|\",\"Proportionally Predicted Label:\",proportion_pred.numpy())\n",
    "        \n",
    "        print(\"Neuron Label Assignments:\")\n",
    "        for idx in range(assignments.numel()):\n",
    "            print(\n",
    "                \"\\t Output Neuron[\",idx,\"]:\",assignments[idx],\n",
    "                \"Proportions:\",proportions[idx],\n",
    "                \"Rates:\",rates[idx]\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "    #####################################\n",
    "print(\"Accuracy:\", num_correct / len(encoded_test_inputs) )"
   ]
  },
  {
   "source": [
    "## Learning Rules\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "#### [Basic STDP Model:](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity)\n",
    "\n",
    "The weight change $\\Delta w_j$ of a synapse from a presynaptic neuron $j$| depends on the relative timing between presynaptic spike arrivals and postsynaptic spikes. \n",
    "\n",
    "Presynaptic spike arrival times at synapse $j$ are denoted by $t^f_j$ where $f$=1,2,3,... counts the presynaptic spikes. \n",
    "\n",
    "Postsynaptic firing times are denoted by $t^n_i$ where $n$=1,2,3,... counts the postsynaptic spikes. \n",
    "\n",
    "The total weight change $\\Delta w_j$ induced by a stimulation protocol with pairs of pre- and postsynaptic spikes is then:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\sum_{f=1}^{N} \\sum_{n=1}^{N} W (t_i^n - t_j^f)\n",
    "$$\n",
    "\n",
    "where **$W(x)$** denotes one of the STDP functions (also called learning window).\n",
    "\n",
    "A popular choice for the STDP function **$W(x)$**\n",
    "$$\n",
    "W(x)=A_+e^{−x/\\tau+} \\hspace{5mm} for \\hspace{5mm} x > 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "W(x)=−A_−e^{x/\\tau−} \\hspace{5mm} for \\hspace{5mm} x < 0 \n",
    "$$\n",
    "\n",
    "The parameters A+ and A− may depend on the current value of the synaptic weight $w_j$. The time constants are on the order of $\\tau_+$ = 10ms and $\\tau_-$=10ms\n",
    "\n",
    "In summary: \n",
    "\n",
    "The weight change $\\Delta w_j$ will be a decreasing positive value the more $t^n_i$ (post synaptic firing time) exceedes $t^f_j$ (presynaptic spike time). This is also referred to as long-term potentiation (LTP).\n",
    "\n",
    "The weight change $\\Delta w_j$ will be a decreasing negative value the more $t^f_j$ (presynaptic spike time) exceedes $t^n_i$ (post synaptic firing time). This is also referred to as long-term depression (LTD).\n",
    "\n",
    "Source: http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PostPre\n",
    "\n",
    "#### Summary\n",
    "\n",
    "A simple STDP rule involving both pre- and post-synaptic spiking activity. By default, pre-synaptic update is negative and the post-synaptic update is positive.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t) - \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t)\n",
    "$$\n",
    "\n",
    "Where $S_{pre}(t)$ and $S_{post}(t)$ indicate if there was a spike at time t for either the pre-synaptic or post-synaptic neurons. \n",
    "\n",
    "Additionally $t_{pre}$ is the timestamp when the pre-synaptic neuron last fired, and $t_{post}$ is the timestamp when the post-synaptic neuron last fired.\n",
    "\n",
    "The `trace_decay` value specified when creating a new `Nodes` layer is given by:\n",
    "\n",
    "$$\n",
    "trace\\_decay = {e^{-\\frac{1}{\\tau}}}\n",
    "$$\n",
    "\n",
    "The value $\\Delta w$ is calculated and applied at each synapse for every timestep."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### WeightDependentPostPre\n",
    "\n",
    "#### Summary\n",
    "\n",
    "STDP rule involving both pre- and post-synaptic spiking activity. The post-synaptic update is positive and the pre- synaptic update is negative, and both are dependent on the magnitude of the synaptic weights.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t)(w_{max} - w) - \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t) (w - w_{min})\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Hebbian\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Simple Hebbian learning rule. Pre- and post-synaptic updates are both positive.\n",
    "\n",
    "The rule follows the equation below for each timestep $t$:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} (t) = \\eta_1 (e^{\\frac{t - t_{pre}}{\\tau}})S_{post}(t) + \\eta_0 (e^{\\frac{t - t_{post}}{\\tau}})S_{pre}(t)\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}